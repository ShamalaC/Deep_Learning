{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Step 1: Collect a Source Video and Divide into Frames"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python\n",
      "  Downloading opencv_python-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/anaconda3/lib/python3.11/site-packages (from opencv-python) (1.26.4)\n",
      "Downloading opencv_python-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl (54.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 MB\u001b[0m \u001b[31m21.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-python\n",
      "Successfully installed opencv-python-4.10.0.84\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting opencv-python-headless\n",
      "  Downloading opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl.metadata (20 kB)\n",
      "Requirement already satisfied: numpy>=1.21.2 in /opt/anaconda3/lib/python3.11/site-packages (from opencv-python-headless) (1.26.4)\n",
      "Downloading opencv_python_headless-4.10.0.84-cp37-abi3-macosx_11_0_arm64.whl (54.8 MB)\n",
      "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.8/54.8 MB\u001b[0m \u001b[31m16.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m00:01\u001b[0m00:01\u001b[0m\n",
      "\u001b[?25hInstalling collected packages: opencv-python-headless\n",
      "Successfully installed opencv-python-headless-4.10.0.84\n",
      "Note: you may need to restart the kernel to use updated packages.\n"
     ]
    }
   ],
   "source": [
    "pip install opencv-python-headless"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "4.10.0\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "# This will print the OpenCV version if the installation was successful\n",
    "print(cv2.__version__)  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "309 frames extracted.\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import os\n",
    "\n",
    "# Creating a directory to store video frames\n",
    "if not os.path.exists('video_frames'):\n",
    "    os.mkdir('video_frames')\n",
    "\n",
    "# Loading the source video\n",
    "video_path = 'source.mp4' \n",
    "vidcap = cv2.VideoCapture(video_path)\n",
    "success, image = vidcap.read()\n",
    "count = 0\n",
    "\n",
    "# Loop through the video and save each frame as an image\n",
    "while success:\n",
    "    # Save frame as JPEG file\n",
    "    cv2.imwrite(f\"video_frames/frame{count:04d}.jpg\", image)  \n",
    "    success, image = vidcap.read()\n",
    "    count += 1\n",
    "\n",
    "print(f\"{count} frames extracted.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. cv2.VideoCapture() opens the video file.\n",
    "2. The while success: loop reads each frame, and cv2.imwrite() saves each frame as a JPEG image in the video_frames directory.\n",
    "4. Each frame is saved with a numbered file name like frame0001.jpg, frame0002.jpg, etc."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Divided the video into discrete image frames."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 2 : Conducting inference on each frame of the video, drawing bounding boxes around detected vehicles."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processed 309 frames.\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "from torchvision.models.detection import fasterrcnn_resnet50_fpn\n",
    "from torchvision.transforms import functional as F\n",
    "from PIL import Image\n",
    "import cv2\n",
    "\n",
    "# Load the pre-trained Faster R-CNN model\n",
    "model = fasterrcnn_resnet50_fpn(pretrained=True)\n",
    "model.eval()\n",
    "\n",
    "# Check if a GPU is available, if not use the CPU\n",
    "device = torch.device('cuda') if torch.cuda.is_available() else torch.device('cpu')\n",
    "model.to(device)\n",
    "\n",
    "# COCO traffic-related label indices with person and other objects\n",
    "TRAFFIC_LABELS = {\n",
    "    1: 'person', \n",
    "    3: 'car', \n",
    "    6: 'bus', \n",
    "    8: 'truck', \n",
    "    4: 'motorbike', \n",
    "    2: 'bicycle', \n",
    "    10: 'traffic light', \n",
    "    12: 'stop sign'\n",
    "}\n",
    "def detect_vehicles(image_path):\n",
    "    # Load image using PIL\n",
    "    image = Image.open(image_path).convert(\"RGB\")\n",
    "\n",
    "    # Transform image to tensor\n",
    "    image_tensor = F.to_tensor(image).unsqueeze(0).to(device)\n",
    "\n",
    "    # Perform inference\n",
    "    with torch.no_grad():\n",
    "        predictions = model(image_tensor)\n",
    "\n",
    "    # Extract bounding boxes, labels, and scores\n",
    "    boxes = predictions[0]['boxes'].cpu().numpy()\n",
    "    labels = predictions[0]['labels'].cpu().numpy()\n",
    "    scores = predictions[0]['scores'].cpu().numpy()\n",
    "\n",
    "    return boxes, labels, scores\n",
    "\n",
    "def draw_boxes_for_traffic_objects(image_path, boxes, labels, scores, threshold=0.5):\n",
    "    image = cv2.imread(image_path) \n",
    "    for i, box in enumerate(boxes):\n",
    "        # Only draw boxes for traffic-related objects with a score above the threshold\n",
    "        if labels[i] in TRAFFIC_LABELS and scores[i] > threshold:\n",
    "            # Extract bounding box coordinates\n",
    "            x1, y1, x2, y2 = map(int, box)\n",
    "            # Draw bounding box\n",
    "            cv2.rectangle(image, (x1, y1), (x2, y2), (0, 255, 0), 2)\n",
    "            # Add label to the bounding box\n",
    "            cv2.putText(image, TRAFFIC_LABELS[labels[i]], (x1, y1 - 10), cv2.FONT_HERSHEY_SIMPLEX, 0.9, (255, 0, 0), 2)\n",
    "\n",
    "    return image\n",
    "\n",
    "# Process all frames\n",
    "output_dir = 'output_frames'\n",
    "if not os.path.exists(output_dir):\n",
    "    os.mkdir(output_dir)\n",
    "\n",
    "# List all frames\n",
    "frame_list = sorted(os.listdir('video_frames'))\n",
    "\n",
    "# Run detection and save results\n",
    "for frame_file in frame_list:\n",
    "    frame_path = os.path.join('video_frames', frame_file)\n",
    "    boxes, labels, scores = detect_vehicles(frame_path)\n",
    "    result_frame = draw_boxes_for_traffic_objects(frame_path, boxes, labels, scores)\n",
    "    cv2.imwrite(os.path.join(output_dir, frame_file), result_frame)\n",
    "\n",
    "print(f\"Processed {len(frame_list)} frames.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. Faster R-CNN is a object detection model utlized to detect objects in an image and draw bounding boxes around them. The pretrained=True flag loads weights from a model pre-trained on the COCO dataset.\n",
    "\n",
    "2. model.eval() puts the model into evaluation mode, which is important for inference to ensure layers like dropout and batch normalization behave correctly.\n",
    "labels represent COCO dataset class indices for common traffic-related objects like person, car, bus, truck, etc. They are used later for drawing bounding boxes on the detected objects.\n",
    "\n",
    "3. Loading the image: The image is loaded and converted to RGB format using PIL.\n",
    "4. Converting to a tensor: The image is transformed into a PyTorch tensor.\n",
    "5. Performing inference: The model is applied to the image tensor to make predictions. torch.no_grad() ensures no gradients are computed, speeding up inference and saving memory.\n",
    "6. Extracting results: The model outputs bounding boxes (boxes), object labels (labels), and confidence scores (scores), which are converted to NumPy arrays for easy manipulation.\n",
    "7. Thresholding: Only objects with a confidence score higher than the threshold (0.5 by default) are considered.\n",
    "8. Drawing boxes: For each detected object, the bounding box coordinates (x1, y1, x2, y2) are drawn on the image using cv2.rectangle(), and the corresponding traffic object label is added using cv2.putText().\n",
    "\n",
    "9. Processing the frames: Each frame from the video is loaded from the video_frames folder. The detect_vehicles() function is applied to the frame to get the bounding boxes, labels, and scores.\n",
    "10. Saving the results: After drawing bounding boxes, the modified frames are saved in the output_frames directory."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Step 3 : Formating the Results back into a Video."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Video saved to detected_vehicles_video.mp4\n"
     ]
    }
   ],
   "source": [
    "def frames_to_video(output_dir, output_video_path, fps=30):\n",
    "    frame_files = sorted([f for f in os.listdir(output_dir) if f.endswith('.jpg')])\n",
    "    first_frame = cv2.imread(os.path.join(output_dir, frame_files[0]))\n",
    "    height, width, layers = first_frame.shape\n",
    "\n",
    "    # Define the codec and create VideoWriter object\n",
    "    video = cv2.VideoWriter(output_video_path, cv2.VideoWriter_fourcc(*'mp4v'), fps, (width, height))\n",
    "\n",
    "    for frame_file in frame_files:\n",
    "        frame = cv2.imread(os.path.join(output_dir, frame_file))\n",
    "        video.write(frame)\n",
    "\n",
    "    video.release()\n",
    "    print(f\"Video saved to {output_video_path}\")\n",
    "\n",
    "# Combine frames into video\n",
    "output_video_path = 'detected_vehicles_video.mp4'\n",
    "frames_to_video('output_frames', output_video_path)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "1. This above function takes individual image frames from a specified directory, reads them sequentially, and writes them into a video file using OpenCV's VideoWriter.\n",
    "\n",
    "2. The output is a video that plays the frames at a specified frame rate (default 30 fps), making it ideal for scenarios like object detection where each frame represents a step in a process. "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
